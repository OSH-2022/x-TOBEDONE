## x-TOBEDONE Lab4

### 小组成员与选题

小组成员为：

+ 关浩祥 PB20050978
+ 牛午甲 PB20111656
+ 谭骏飞 PB20061276
+ 徐    奥 PB20061343
+ 赵子毅 PB20051107

我们的选题是 **Ray**

### 测试任务

测试程序为使用计数近似求 $\pi$，原理公式为 $\frac{\pi}{4}=\frac{1}{1}-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\frac{1}{9}-\frac{1}{11}+\dots$

根据这个公式，我们可以连续计算 $10^9$ 个值来得到 $\pi$ 的近似值

核心代码为：

```python
for i in range(0, 1000000000):
            pi += 4.0 * (1 - (i % 2) * 2) / (2 * i + 1)
```

这个代码的特点是大量的加法运算

### 性能指标列表

+ CPU 利用率：反映了 CPU 的利用效率、
+ 吞吐量：单位时间内完成作业的数量
+ 响应时间：指从任务提交到任务第一次产生响应的时间，此时任务可能仍在进行中，但是对于用户而言，他会希望自己提交的请求尽早地开始被系统服务、回应，用户会更多关注系统的响应时间
+ 周转时间：指任务提交到作业完成为止的时间
+ 内存使用率
+ 可并行的 worker 数量

我们选用的指标：周转时间，CPU 利用率，内存使用率。周转时间反映了计算性能，尤其在使用 Ray 实现多任务并发执行时，周转时间是衡量 Ray 对程序性能提升程度的重要指标。而 CPU 利用率和内存使用率可以反应任务对资源的占用情况，而且如果程序在运行过程中如果 CPU 利用率比较低，那么说明程序仍有较大的优化空间，可通过拆分任务增大并行性来进行加速

#### 单机部署和性能测试

部署详见本目录下的“部署文档”

使用 Ray 实现将 $10^9$ 个计算拆分为 100 组，不同组可以并行执行

带有 Ray 的核心代码为：

```python
@ray.remote
class Worker(object):
    def __init__(self):
        self.r = 0.0

    def calculatePiFor(self, start, elements):
        print("begin to run")
        timenow = time.time()
        for i in range(start, start + elements, 2):
            self.r += (2.0/((2*i+1)*(2*i+3)))
        print(time.time() - timenow)
        print("finish run")
        return self.r

    def result(self):
        return self.r
```

```python
for i in range(100):
    # 初始化子任务（Actor）
    worker = Worker.remote()
    future = worker.calculatePiFor.remote(i * 10000000, 10000000)
    futures.append(future)

# 等待所有子任务结果
results = ray.get(futures)
print(4.0*sum(results))
```

测试结果

|          | 程序启动前CPU利用率 | 程序启动后CPU利用率 | 程序启动前内存使用率 | 程序启动后内存使用率 | 程序总运行时间 |
| :------: | :-----------------: | :-----------------: | :------------------: | :------------------: | :------------: |
|  无 Ray  |        2.3%         |        15.5%        |         31%          |         39%          |     95.87s     |
| Ray 版本 |        2.5%         |        97.9%        |         31%          |         76%          |     46.39s     |

### 分析和优化

+ 串行加法：$\frac{1}{1}-\frac{1}{3}=\frac{2}{1*3}$ ，可通过此式将两个循环合并，不过这个优化是涉及具体代码，通过修改代码提升性能，故接下来的分析中并没有将这一项优化加到“性能提升百分比”中

+ 通过 Ray 提供的监控网站可以看到，单机 Ray 共 8 核，多个 worker 是被调度到这 8 个核上，本程序将原来 $10^9$ 次运算拆分为 100 组，生成了 100 个 actor，需要 100 个 worker，调度在 8 个核上运行。大量的调度过程会有比较多的上下文切换，浪费了 CPU 时间。可以考虑将 $10^9$ 次运算拆分为 8 组，减少切换引入的性能损耗。

  这一项优化后的程序运行指标如下：

  |        | 程序启动前CPU利用率 | 程序启动后CPU利用率 | 程序启动前内存使用率 | 程序启动后内存使用率 | 程序总运行时间 |
  | :----: | :-----------------: | :-----------------: | :------------------: | :------------------: | :------------: |
  | 优化前 |        2.5%         |        99.9%        |         31%          |         80%          |     47.19s     |
  | 优化后 |        2.1%         |        98.5%        |         33%          |         49%          |     32.94s     |

​	可以看出，从运行时间来看，优化后性能是优化前的 $\frac{47.19}{32.94}=1.4326$，性能提升了 43.26%. 两种版本下，CPU 利用率都接近 100%. 而内存使用率，优化后的内存使用率大幅下降，是由于大大减少了上下文切换。

​	运行时资源监控截图如下：

​	优化前：

​	![image-20220702092345030](image\image-20220702092345030.png)

​	优化后：

![](image\image-20220702092744593.png)

​	运行时间：

​	优化前：

![image-20220702110238160](image\image-20220702110238160.png)

​	优化后：

![image-20220702110007340](D:\desktop\OS\大实验Git\x-TOBEDONE\Lab4\image\image-20220702110007340.png)

### 分布式部署与性能测试

部署详见本目录下的“部署文档”

在实际实验中，我们使用了分别位于三台电脑上的三台虚拟机，连在同一个内网中

![image-20220702104348943](image\image-20220702104348943.png)

共有 20 个核，所以将 $10^9$ 次计算任务切分为 20 份，运行效果如下：

资源占用情况：

![image-20220702105053524](image\image-20220702105053524.png)

运行时间：

![image-20220702105142402](image\image-20220702105142402.png)

最开始我们是用同一台电脑上的多台虚拟机建立 Ray 集群，因为位于同一台物理机，所以它们天然地处在同一个内网中，无需额外配置。但在实际运行中，运行效率并没有提升，反而略有下降。分析其原因，我们认为是由于每台虚拟机都占有了主机的 8 个核，虽然形式上多台虚拟机加在一起有了更多核心，但是物理上仍是 8 个核，而且不同虚拟机的任务调度到物理机核心上还会产生调度开销，不同 ray 节点之间传递信息还会有消息传输延迟，所以实际运行效率不升反降。

### Docker 部署与测试



### 发布到公开媒体

